{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Code To Turn Image into Wireframe**","metadata":{}},{"cell_type":"code","source":"import cv2\nimport mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nimport numpy as np\nfrom mediapipe import solutions\nfrom mediapipe.framework.formats import landmark_pb2\nfrom skimage import io\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMARGIN = 10  # pixels\nFONT_SIZE = 1\nFONT_THICKNESS = 1\nHANDEDNESS_TEXT_COLOR = (88, 205, 54) # vibrant green\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_landmarks_on_image(rgb_image, detection_result):\n  hand_landmarks_list = detection_result.hand_landmarks\n  handedness_list = detection_result.handedness\n  annotated_image = np.copy(rgb_image)\n\n  # Loop through the detected hands to visualize.\n  for idx in range(len(hand_landmarks_list)):\n    hand_landmarks = hand_landmarks_list[idx]\n    handedness = handedness_list[idx]\n\n    # Draw the hand landmarks.\n    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n    hand_landmarks_proto.landmark.extend([\n      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n    ])\n    solutions.drawing_utils.draw_landmarks(\n      annotated_image,\n      hand_landmarks_proto,\n      solutions.hands.HAND_CONNECTIONS,\n      solutions.drawing_styles.get_default_hand_landmarks_style(),\n      solutions.drawing_styles.get_default_hand_connections_style())\n\n \n\n  return annotated_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mp_drawing = mp.solutions.drawing_utils\nmp_hands = mp.solutions.hands\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nsave_path = '/Users/dylanpallickara/Documents/personal_hand_images'\nimage_path = '/Users/dylanpallickara/Documents/personal_hand_images/Y.jpeg'\nimage = cv2.imread(image_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_options = python.BaseOptions(model_asset_path='/Users/dylanpallickara/Documents/Models/hand_landmarker.task')\noptions = vision.HandLandmarkerOptions(base_options=base_options,\n                                       num_hands=2)\ndetector = vision.HandLandmarker.create_from_options(options)\n\n\nblack = np.zeros(image.shape , np.uint8)\nmp_image = mp.Image.create_from_file(image_path)\n\n\ndetection_result = detector.detect(mp_image)\n\nimage = cv2.imread(image_path)\n\nannotated_image = draw_landmarks_on_image(image, detection_result)\nio.imsave(save_path + 'Y_annotated.jpg', (annotated_image))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2.Import Libraries**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nimport pathlib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Set up Datasets for Training, Testing, and Validation**","metadata":{}},{"cell_type":"code","source":"data_dir = pathlib.Path('/Users/dylanpallickara/Documents/processed_combine_asl_dataset')\n\n\nbatch_size = 256\nimg_height = 400\nimg_width = 400\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nclass_names = train_ds.class_names\nprint(class_names)\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\nnormalization_layer = layers.Rescaling(1./255)\n\nnum_classes = len(class_names)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Create the Sequential Model**","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.summary()\n\nepochs=12\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Visualize the Results and Accuracy of the Model**","metadata":{}},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}